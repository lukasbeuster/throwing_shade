# Sun Blocked: A Spatiotemporal Urban Shade Simulation Pipeline

**Throwing Shade** is a Python-based pipeline for simulating and analyzing the impact of urban shade from buildings and trees on any geolocated dataset.

The tool enhances your existing timeseries data (e.g., GPS tracks, sensor readings) with high-resolution shade metrics. It uses data from the Google Solar API, tree masks generated by a segmentation model (SAM), and Digital Surface Models (DSMs) to calculate shade values for every point at every timestamp in your dataset. By default, the pipeline simulates shade per tile from sunrise to latest timestamp in the tile for each day.

This project is structured as a powerful Command-Line Interface (CLI) that guides the user through the necessary steps, from data acquisition to final analysis.

âš ï¸ **Contains ongoing work**

Please report bugs, suggest improvements and give feedback in the form [here](https://docs.google.com/forms/d/e/1FAIpQLSeT1n7mWViNFB5IKgAt-cFujYCxFqdH0tRDy9AEvzQlUAN5-A/viewform?usp=dialog)!

-----

## Features

  - **Step-by-Step CLI**: An intuitive command-line interface guides you through the entire workflow.
  - **Interactive Coverage Check**: Visualize the required Solar API tiles for your dataset and adjust parameters *before* downloading gigabytes of data.
  - **Automated Data Fetching**: Downloads all necessary building, terrain, and RGB data from the Google Solar API and OpenStreetMap.
  - **AI-Powered Tree Segmentation**: Uses the Segment Anything Model (SAM) from Meta AI to generate accurate tree canopy masks from aerial imagery.
  - **Advanced Shade Simulation**: Computes detailed shade rasters, including instantaneous shade and time-averaged shadow fractions.
  - **Parallel Processing**: Leverages multiple CPU cores to significantly speed up heavy processing tasks.

-----

## How It Works

The pipeline is broken down into five distinct, sequential steps, each run by a simple command. This modular approach allows you to inspect the output of each stage and re-run steps with different parameters without starting from scratch.

1.  **`check`**: Analyzes your dataset and determines the geographic tiles needed for the analysis.
2.  **`download`**: Fetches all raw data (DSMs, imagery, building footprints) for the selected tiles.
3.  **`segment`**: Runs the AI model to identify trees and create canopy masks.
4.  **`process-rasters`**: A heavy-lifting step that processes the raw DSMs into analysis-ready terrain and canopy models.
5.  **`process-shade`**: The final step, which runs the shade simulation and merges the results back into your original dataset.

## ğŸš€ Getting Started

### 1\. Installation

First, clone the pipeline branch of the repository to your local machine:

```bash
git clone --branch feature/cli-refactor https://github.com/lukasbeuster/throwing_shade.git
cd throwing_shade
```

Next, it's highly recommended to create a dedicated Python environment to avoid conflicts.

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows, use: .venv\Scripts\activate
```

Install all the required dependencies using pip:

```bash
pip install -r requirements.txt
```

### 2\. Configuration

Before running the pipeline, you must configure it for your project.

**A. Set Up Your API Key**

The pipeline requires a Google Solar API key.

Open the `.env` file and paste in your API key:
```
GOOGLE_API_KEY=YOUR_API_KEY_HERE
```

**B. Download Required Files**

Download the Solar API coverage shapefiles and Segment Anything Model.

1.  Download medium and high resolution coverage shapefiles [here](https://developers.google.com/maps/documentation/solar/coverage)
2.  Download SAM ViT-H checkpoint [here](https://github.com/facebookresearch/segment-anything#model-checkpoints)


**C. Configure Your Run**
1.  Copy the main configuration template:
    ```bash
    cp config.yaml.template config.yaml
    ```
2.  Open `config.yaml` and edit the paths and parameters. **At a minimum, you must set:**
      * `dataset_path`: The full path to your input data file.
      * `dependencies`: The full paths to your Solar API coverage shapefiles and the SAM model checkpoint.
      * `columns`: The column names matching the required columns.
      * `seasons`:
        * `summer` (dict): Input the UTC for the summer period (daylight savings period). DO NOT change the DST value from 0.
        * `winter` (dict): Input the UTC for the winter period (not daylight savings period). DO NOT change the DST value from 0.
      * `year_configs`:
        * `year` (dict): For each timestamp year present in the dataset input the solstice day, the daylight savings start and end date in the form "yyyy-mm-dd".

    **Other parameters:**
    * `simulation`:
      * `shade_interval_minutes` (int): The frequency of shade simulation in minutes. Default is 30 minutes.
      * `combined_shade`, `building_shade` (Boolean): Control whether to simulate combined and/or building shade. Combined shade is shade from both buildings and trees.
      * `start_time` (str): Can provide a start time in the 'HH:MM' format if you don't want to simulate from sunrise to timestamp.
      * `bin_size` (int): The bin size parameter can be used if the dataset is very large and the computation will take too much time. It is defined as an integer number of days for setting a maximum difference in number of days from point timestamp to simulation date.
      * `simulate_solstice` (boolean): Choose whether to simulate solstice day even if not necessary for the dataset timestamps. It can act as a good reference point.
      * `buffers` (list): List of buffer values for extracting the shade results around each data point. Since shade simulation is very location sensitive, depending on the location accuracy can use a buffer. For no buffer, input '[0]'.
    * `seasons`:
      * `summer` (dict): Input the tree transmissivity % (TRS) for the summer period (daylight savings period). DO NOT change the DST value from 0. A good default for TRS is 10% for summer.
      * `winter` (dict): Input the tree transmissivity for the winter period (daylight savings period). DO NOT change the DST value from 0. A good default for TRS is 45-50% in winter.
    * `solar_api`:
      * `min_points_per_tile` (int): Choose minimum number of points required per tile for it to be included in the analysis and download the Solar API layers.
    * `max_workers` (int): Maximum number of CPU cores to use for parallel processing.
    * `extra_outputs`:
      * `building_shade_step` (boolean): Put True if you want the building shade value at the point timestamp as an output parameter, False otherwise.
      * `tree_shade_step` (boolean): Put True if you want the combined shade value at the point timestamp as an output parameter, False otherwise.
      * `bldg_shadow_fraction` (boolean): Put True if you want the accumulated building shade value until the point timestamp as an output parameter, False otherwise.
      * `tree_shadow_fraction` (boolean): Put True if you want the accumulated combined shade value until the point timestamp as an output parameter, False otherwise.
      * `hours_before` (list): Input list of integers to have the cumulative shade in the x hours before point timestamp as an output parameter. Otherwise put [].


-----

## ğŸ’» Usage: Running the Pipeline

All commands are run from your terminal in the project's root directory.

### See All Commands

To see a full list of available commands and their options, run:

```bash
python pipeline.py --help
```

### Step 1: Check Coverage (Interactive)

This crucial first step analyzes your dataset and shows you which geographic tiles you need to download. It allows you to adjust the density of points required per tile, giving you control over the cost and scope of your analysis. Solar API quotas and pricing can be checked [here](https://mapsplatform.google.com/pricing/?_gl=1*14kp531*_ga*NzgzMjQ1MzIyLjE3NDQ3OTI2MzM.*_ga_NRWSTWS78N*czE3NTE1NDcxOTEkbzEkZzEkdDE3NTE1NDgwNjgkajQ1JGwwJGgw&utm_experiment=13102542) under Solar API Data Layers. For small and low spread datasets, first try with minimum number of points as 1.

```bash
python pipeline.py check --min-points 10
```

  * **Action Required:** The script will save a `coverage_preview_...geojson` file in the `output/` directory. **You must open this file in a GIS viewer** (like the "GeoJSON" extension in VS Code, QGIS, ArcGIS) to inspect the tile layout.
  * The script will then pause and ask for your confirmation. If the number of tiles is too high, cancel the operation (`N`), and re-run the `check` command with a higher `--min-points` value.

Output: Coverage preview file in `{output_dir}/step1_solar_coverage`

### Step 2: Download Data

Once you are satisfied with the coverage, run the download command. This will fetch all the necessary data from Solar API (digital surface models, RGB images and building masks) and generate a unique `osmid` (a run ID) for your project. Take note of this ID for future.

```bash
python pipeline.py download
```

Output: 4 rasters files per tile in `{output_dir}/step2_solar_data/{osmid}`

### Step 3: Segment Trees

This step runs the tree segmentation model on the downloaded RGB imagery. The segmented tree masks are saved as rasters.

```bash
python pipeline.py segment
```
Output: Segmented raster per tile in `{output_dir}/step3_segmentation/{osmid}`

### Step 4: Process Rasters

This is a time-consuming step that prepares the raw DSMs into analysis-ready terrain and canopy models. You only need to run this once per dataset.

```bash
python pipeline.py process-rasters
```

Output: CHM and DTM raster per tile in `{output_dir}/step4_raster_processing/{osmid}`

### Step 5: Process Shade

This is the final step. It runs the shade simulation using the prepared data and merges the results back into your original dataset.

```bash
python pipeline.py process-shade
```

Your final, shade-enhanced dataset will be saved in `{output_dir}/step5_final_results/{osmid}/{file_name}.geojson`.

### Run All Steps Automatically

For automated workflows, you can execute the entire pipeline with a single command. This will skip the interactive confirmation step and the checkpoints.

```bash
python pipeline.py run-all --min-points 10
```

### Managing Runs (switching & reverting)

Changing `config.yaml` **does not** create a new run. The pipeline uses the current run ID (`osmid`) stored in `results/output/run_info.json`. Use these helpers to manage runs:

```bash
# Show the currently selected run (from run_info.json)
python pipeline.py current-run --config config.yaml

# List all previous runs found under step2_solar_data/
python pipeline.py list-runs --config config.yaml

# Switch back to a previous run by its OSMID
python pipeline.py set-run --config config.yaml <OSMID>

# Start fresh (clears run_info.json; the next `check` will initialize a new run)
python pipeline.py new-run --config config.yaml
```

Tip: For fully isolated experiments, you can also change `output_dir` in `config.yaml` to a new folder, but for most cases the commands above are enough.

-----

## âš™ï¸ Configuration Details

All pipeline parameters are controlled in `config.yaml`.

| Parameter | Section | Description |
| :--- | :--- | :--- |
| `dataset_path` | `Input and Output Paths` | Path to your input csv, GeoJSON or Shapefile with longitude and latitude columns. |
| `output_dir` | `Input and Output Paths` | The base directory where all results will be saved. |
| `columns` | `Dataset Column Names`| Maps the required column names (`latitude`, `longitude`, etc.) to the names in your dataset. Longitude and latitude has to be in decimal degrees. |
| `dependencies` | `Dependency Paths`| Paths to essential files like the SAM model checkpoint and Solar API coverage shapefiles. |
| `simulation` | `Shade Simulation Parameters`| Controls the core simulation settings like shade interval, buffer sizes, and whether to include building/combined shade. |
| `year_configs` | `Daylight Saving Time`| **Crucial for accuracy.** Define the start and end dates of DST for each year present in your data. |

## Folder Structure

```
pipeline.py
â”‚
config.yaml
â”‚
src/
â”œâ”€â”€ (All scripts etc.)
â”œâ”€â”€ .env file
â”‚
output_directory/
â”œâ”€â”€ step1_solar_coverage/
â”‚   â”œâ”€â”€ coverage_previews/
â”‚   â”‚   â””â”€â”€ coverage_preview_{number_of_tiles}_tiles.geojson
â”‚   â””â”€â”€ {OSMID}/
â”‚       â”œâ”€â”€ OSM building polygons
â”‚       â””â”€â”€ query points for SolarAPI
â”œâ”€â”€ step2_solar_data/
â”‚   â””â”€â”€ {OSMID}/
â”‚        â””â”€â”€ solar data rasters
â”œâ”€â”€ step3_segmentation/
â”‚   â””â”€â”€ {OSMID}/
â”‚       â””â”€â”€ segmented RGB files
â”œâ”€â”€ step4_raster_processing/
â”‚   â””â”€â”€ {OSMID}/
â”‚       â””â”€â”€ processed DTM and CHM rasters
â”œâ”€â”€ step5_shade_results/
â”‚   â””â”€â”€ {OSMID}/
â”‚       â”œâ”€â”€ building_shade/
â”‚       â””â”€â”€ combined_shade/
â”‚           â””â”€â”€ {tile_id}/
â”‚               â”œâ”€â”€ {OSMID}_{tile_id}_Shadow_{DATE}_{TIME}_LST.tif
â”‚               â””â”€â”€ {OSMID}_{tile_id}_shadow_fraction_on_{DATE}_{TIME}.tif
â””â”€â”€ step6_final_result/
    â””â”€â”€ {OSMID}/
        â””â”€â”€ final shade enhanced dataset
```

## Cite

Ozberkman, D., van Selm, M., Venverloo, T., and Beuster, L.: Sun Blocked: Integrating Shade into Urban Climate Assessments, 12th International Conference on Urban Climate, Rotterdam, The Netherlands, 7â€“11 Jul 2025, ICUC12-797, https://doi.org/10.5194/icuc12-797, 2025.
