{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Use DeepForest for Object Detection\n",
    "\n",
    "The DeepForest model is primarily used for detecting trees in aerial or satellite imagery. Here’s how you can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepforest import DeepForest\n",
    "from deepforest import get_data\n",
    "import cv2\n",
    "\n",
    "# Load a pre-trained DeepForest model\n",
    "model = DeepForest()\n",
    "model.use_release()\n",
    "\n",
    "# Load an image\n",
    "image_path = get_data(\"../data/clean_data/solar/12011952/12011952_p_1_2022_06_02_rgb.tif\")\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Perform object detection\n",
    "predictions = model.predict_image(image=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict tiles instead of images\n",
    "Large tiles covering wide geographic extents cannot fit into memory during prediction and would yield poor results due to the density of bounding boxes. Often provided as geospatial .tif files, remote sensing data is best suited for the predict_tile function, which splits the tile into overlapping windows, performs prediction on each of the windows, and then reassembles the resulting annotations.\n",
    "\n",
    "Let’s show an example with a small image. For larger images, patch_size should be increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_path = get_data(\"OSBS_029.tif\")\n",
    "# Window size of 300px with an overlap of 25% among windows for this small tile.\n",
    "predicted_raster = model.predict_tile(raster_path, return_plot = True, patch_size=300,patch_overlap=0.25)\n",
    "\n",
    "# View boxes overlayed when return_plot=True, when False, boxes are returned.\n",
    "plt.imshow(predicted_raster)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SAM for Segmentation\n",
    "SAM can use boxes as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "import torch\n",
    "\n",
    "# Load the SAM model\n",
    "sam_checkpoint = \"sam_vit_h.pth\"  # Update with your SAM model checkpoint\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_type = \"vit_h\"  # Example model type\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# Set the image for segmentation\n",
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Object Detection with Segmentation\n",
    "\n",
    "Now, you can combine the outputs from DeepForest and SAM. You might want to segment each detected object (e.g., each tree) separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Iterate through the detected objects and use SAM to segment them\n",
    "for i, prediction in predictions.iterrows():\n",
    "    # Extract the bounding box from the DeepForest predictions\n",
    "    x1, y1, x2, y2 = int(prediction['xmin']), int(prediction['ymin']), int(prediction['xmax']), int(prediction['ymax'])\n",
    "\n",
    "    # You can crop the image around the bounding box or directly use it as a prompt\n",
    "    # Crop the image if necessary\n",
    "    cropped_image = image[y1:y2, x1:x2]\n",
    "\n",
    "    # Create a box prompt for SAM\n",
    "    box = np.array([x1, y1, x2, y2])\n",
    "\n",
    "    # Predict the mask using SAM\n",
    "    masks, _, _ = predictor.predict(box=box)\n",
    "\n",
    "    # If you want to visualize or save the masks\n",
    "    for mask in masks:\n",
    "        cv2.imshow(\"Mask\", mask)\n",
    "        cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Processing and Visualization\n",
    "\n",
    "You can post-process the masks to refine them or combine them with the original image. For example, overlaying the masks on the original image or bounding boxes to see the segmentation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Overlay mask on the original image\n",
    "overlay_image = image.copy()\n",
    "for mask in masks:\n",
    "    overlay_image[mask] = [0, 255, 0]  # Color the masked area\n",
    "\n",
    "cv2.imshow(\"Overlay\", overlay_image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Saving the Results\n",
    "\n",
    "You can save the segmented images or masks for further use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cv2.imwrite(\"segmented_image.png\", overlay_image)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
