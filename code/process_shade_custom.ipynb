{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ff912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import importlib\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.transform import Affine\n",
    "from rasterio.transform import rowcol\n",
    "from rasterio.mask import mask\n",
    "import datetime as dt\n",
    "from osgeo import gdal, osr\n",
    "from osgeo.gdalconst import *\n",
    "import shade_setup as shade\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from shapely.geometry import box\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.ndimage import median_filter\n",
    "from scipy.ndimage import uniform_filter\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage import minimum_filter\n",
    "\n",
    "import startinpy\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "importlib.reload(shade)\n",
    "\n",
    "# Set exception handling\n",
    "gdal.UseExceptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73316e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raster(path, osmid):\n",
    "    try:\n",
    "        print(f\"Starting processing for {path} with OSMID: {osmid}\")\n",
    "\n",
    "        # Process each DSM file\n",
    "        fixed_path = path.replace(\"\\\\\", \"/\")\n",
    "        last_slash_index = fixed_path.rfind(\"/\")\n",
    "        file_name = fixed_path[last_slash_index + 1:]\n",
    "        print(\"File name:\", file_name)\n",
    "\n",
    "        # Define new file paths based on the osmid\n",
    "        file_name_building = f\"C:/Users/Dila Ozberkman/Desktop/AMS Research/Urban Shade/throwing_shade/data/clean_data/solar/{osmid}/rdy_for_processing/{file_name[:-7]}building_dsm.tif\"\n",
    "        file_name_trees = f\"C:/Users/Dila Ozberkman/Desktop/AMS Research/Urban Shade/throwing_shade/data/clean_data/solar/{osmid}/rdy_for_processing/{file_name[:-7]}canopy_dsm.tif\"\n",
    "        # file_name_building = f'../data/clean_data/solar/{osmid}/rdy_for_processing/{file_name[:-7]}building_dsm.tif'\n",
    "        # file_name_trees = f'../data/clean_data/solar/{osmid}/rdy_for_processing/{file_name[:-7]}canopy_dsm.tif'\n",
    "\n",
    "        # List of file paths to check\n",
    "        file_paths = [file_name_building, file_name_trees]\n",
    "\n",
    "        # Check if the files already exist\n",
    "        if check_files_exist(file_paths):\n",
    "            print(\"Files already exist. Skipping creation.\")\n",
    "        else:\n",
    "            # Read DSM\n",
    "            with rasterio.open(path) as src:\n",
    "                dsm_data = src.read(1)\n",
    "                dsm_meta = src.meta.copy()\n",
    "                dsm_crs = src.crs\n",
    "                dsm_bounds = src.bounds\n",
    "                dsm_transform = src.transform\n",
    "                dsm_shape = dsm_data.shape\n",
    "\n",
    "                # Extract further metadata\n",
    "                width = src.width\n",
    "                height = src.height\n",
    "                nodata_value = src.nodata\n",
    "                dtype = src.dtypes[0]\n",
    "\n",
    "                # Calculate resolution\n",
    "                resolution_x = dsm_transform[0]\n",
    "                resolution_y = -dsm_transform[4]  # Typically negative in the geotransform\n",
    "\n",
    "                # Calculate extent\n",
    "                xmin = dsm_transform[2]\n",
    "                ymax = dsm_transform[5]\n",
    "                xmax = xmin + (width * resolution_x)\n",
    "                ymin = ymax + (height * dsm_transform[4])  # Typically negative\n",
    "\n",
    "            # # Create a bounding box polygon from the raster bounds\n",
    "            dsm_bbox = box(dsm_bounds.left, dsm_bounds.bottom, dsm_bounds.right, dsm_bounds.top)\n",
    "            dsm_bbox_gdf = gpd.GeoDataFrame({'geometry': [dsm_bbox]}, crs=dsm_crs)\n",
    "\n",
    "            print(\"Making CHM mask\")\n",
    "\n",
    "            # New CHM mask path, identified by the OSMID and filename in the new folder\n",
    "            chm_mask_folder = f\"C:/Users/Dila Ozberkman/Desktop/AMS Research/Urban Shade/throwing_shade/data/clean_data/canopy_masks/{osmid}/\"\n",
    "            # chm_mask_folder = f'../data/clean_data/canopy_masks/{osmid}/'\n",
    "            chm_mask_file = f'{chm_mask_folder}{file_name[:-7]}rgb_segmented.tif'\n",
    "\n",
    "            if os.path.exists(chm_mask_file):\n",
    "                print(f\"CHM mask found: {chm_mask_file}\")\n",
    "\n",
    "                # Load the CHM mask (assuming itâ€™s already in the same resolution and extent)\n",
    "                with rasterio.open(chm_mask_file) as chm_src:\n",
    "                    chm_mask = chm_src.read(1)  # Assuming it's a single-band mask\n",
    "\n",
    "                # Apply the CHM mask to the DSM data\n",
    "                canopy_dsm = np.where(chm_mask, dsm_data, np.nan)  # Use NaN for masked-out areas\n",
    "\n",
    "            else:\n",
    "                print(f\"CHM mask not found: {chm_mask_file}. Skipping mask application.\")\n",
    "\n",
    "            # Read building_mask\n",
    "            mask_path = path.replace(\"dsm\", \"mask\")\n",
    "\n",
    "            with rasterio.open(mask_path) as src:\n",
    "                bldg_mask = src.read(1)\n",
    "                bldg_mask_meta = src.meta.copy()\n",
    "                bldg_transform = src.transform\n",
    "                bldg_crs = src.crs\n",
    "                bldg_dtype = src.dtypes[0]\n",
    "\n",
    "            # Load corresponding AHN subtiles\n",
    "            # buildings_path = f'../data/clean_data/solar/{osmid}/{osmid}_buildings.gpkg'\n",
    "            buildings_path = f\"C:/Users/Dila Ozberkman/Desktop/AMS Research/Urban Shade/throwing_shade/data/clean_data/solar/{osmid}/{osmid}_buildings.gpkg\"\n",
    "            buildings = gpd.read_file(buildings_path, mask=dsm_bbox_gdf)\n",
    "\n",
    "            # Check if buildings GeoDataFrame is empty\n",
    "            if buildings.empty:\n",
    "                print(\"No buildings found in the mask area.\")\n",
    "                osm_bldg_mask = np.zeros(dsm_data.shape, dtype='uint8')  # Create an empty mask\n",
    "            else:\n",
    "                if buildings.crs != dsm_crs:\n",
    "                    buildings = buildings.to_crs(dsm_crs)\n",
    "\n",
    "                # Buffer to combat artefacts.\n",
    "                buildings.geometry = buildings.buffer(1.5)\n",
    "\n",
    "                # Rasterize building polygons (same size as dsm so it works with UMEP)\n",
    "                print(\"Rasterizing building polygons\")\n",
    "                try:\n",
    "                    osm_bldg_mask = rasterize(\n",
    "                        ((mapping(geom), 1) for geom in buildings.geometry),\n",
    "                        out_shape=dsm_data.shape,\n",
    "                        transform=dsm_meta['transform'],\n",
    "                        fill=0,\n",
    "                        dtype='uint8'\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during rasterization: {e}\")\n",
    "                    osm_bldg_mask = np.zeros(dsm_data.shape, dtype='uint8')  # Create an empty mask in case of failure\n",
    "\n",
    "            combined_building_mask = np.logical_or(bldg_mask, osm_bldg_mask).astype(np.uint8)\n",
    "            combined_bldg_tree_mask = np.logical_or(chm_mask, combined_building_mask).astype(np.uint8)\n",
    "\n",
    "            dtm_raw = np.where(combined_bldg_tree_mask == 0, dsm_data, np.nan)\n",
    "\n",
    "            print(\"Filtering data\")\n",
    "\n",
    "            ### Filter the raw data\n",
    "            ## Apply minimum filter\n",
    "            filtered_data = apply_minimum_filter(dtm_raw, np.nan, size=50)\n",
    "            # filtered_data = apply_minimum_filter(filtered_data, np.nan, size=50)\n",
    "            filtered_data = apply_minimum_filter(filtered_data, np.nan, size=30)\n",
    "            filtered_data = apply_minimum_filter(filtered_data, np.nan, size=10)\n",
    "\n",
    "            ### Interpolate:\n",
    "\n",
    "            print(\"Doing Laplace interpolation\")\n",
    "\n",
    "            t = dsm_transform\n",
    "            pts = []\n",
    "            coords = []\n",
    "            for i in range(filtered_data.shape[0]):\n",
    "                for j in range(filtered_data.shape[1]):\n",
    "                    x = t[2] + (j * t[0]) + (t[0] / 2)\n",
    "                    y = t[5] + (i * t[4]) + (t[4] / 2)\n",
    "                    z = filtered_data[i][j]\n",
    "                    # Add all point coordinates. Laplace interpolation keeps existing values.\n",
    "                    coords.append([x,y])\n",
    "                    if not np.isnan(z):\n",
    "                        pts.append([x, y, z])\n",
    "                        # print('data found')\n",
    "            dt = startinpy.DT()\n",
    "            dt.insert(pts, insertionstrategy=\"BBox\")\n",
    "\n",
    "            interpolated = dt.interpolate({\"method\": \"Laplace\"}, coords)\n",
    "\n",
    "            if interpolated.shape != dsm_data.shape:\n",
    "                print(\"Interpolation is fucking it up\")\n",
    "                print(f\"Interpolated shape: {interpolated.shape}\")\n",
    "                print(f\"Initial dsm shape: {dsm_data.shape}\")\n",
    "\n",
    "            # Calculate the number of rows and columns\n",
    "            ncols = int((xmax - xmin) / resolution_x)\n",
    "            nrows = int((ymax - ymin) / resolution_y)\n",
    "\n",
    "            # Create an empty raster array\n",
    "            raster_array = np.full((nrows, ncols), np.nan, dtype=np.float32)\n",
    "\n",
    "            # Ensure the points are in the correct structure (startinpy returns a flattened 1D array containing only the interpolated values for some reason)\n",
    "\n",
    "            # Combine the coordinates and values into a 2D array with shape (n, 3)\n",
    "            points = np.array([(x, y, val) for (x, y), val in zip(coords, interpolated)])\n",
    "            points\n",
    "\n",
    "            # Check if the array length is a multiple of 3\n",
    "            if points.size % 3 != 0:\n",
    "                raise ValueError(f\"Array size {points.size} is not a multiple of 3, cannot reshape.\")\n",
    "            # Reshape the points array if it's flattened\n",
    "            if points.ndim == 1:\n",
    "                points = points.reshape(-1, 3)\n",
    "\n",
    "            # In this example, points should be a 2D array with shape (n, 3)\n",
    "            print(f\"Points shape: {points.shape}\")\n",
    "\n",
    "            # Map the points to the raster grid\n",
    "            for point in points:\n",
    "                if len(point) != 3:\n",
    "                    raise ValueError(f\"Expected point to have 3 elements (x, y, value), but got {len(point)} elements.\")\n",
    "                x, y, value = point\n",
    "                # Skip points with NaN values\n",
    "                if np.isnan(value):\n",
    "                    continue\n",
    "\n",
    "                col = int((x - xmin) / resolution_x)\n",
    "                row = int((ymax - y) / resolution_y)\n",
    "\n",
    "                # Ensure the indices are within bounds\n",
    "                if 0 <= col < ncols and 0 <= row < nrows:\n",
    "                    raster_array[row, col] = value\n",
    "\n",
    "            # Define the transform (mapping from pixel coordinates to spatial coordinates)\n",
    "            transform = from_origin(xmin, ymax, resolution_x, resolution_y)\n",
    "\n",
    "            # print(transform)\n",
    "\n",
    "            # Define the metadata for the new raster\n",
    "            meta = {\n",
    "                'driver': 'GTiff',\n",
    "                'dtype': dtype,\n",
    "                'nodata': nodata_value,\n",
    "                'width': width,\n",
    "                'height': height,\n",
    "                'count': 1,\n",
    "                'crs': dsm_crs,\n",
    "                'transform': transform\n",
    "            }\n",
    "\n",
    "            post_interpol_filter = apply_minimum_filter(raster_array, np.nan, size=40)\n",
    "            post_interpol_filter = apply_minimum_filter(post_interpol_filter, np.nan, size=20)\n",
    "\n",
    "            dsm_buildings = np.where(combined_building_mask == 0, post_interpol_filter, dsm_data)\n",
    "\n",
    "            # Save building dsm and canopy dsm\n",
    "            print(\"Saving DSM and Canopy DSM\")\n",
    "\n",
    "            # Find the index of the last '/' character\n",
    "            path = path.replace(\"\\\\\", \"/\")\n",
    "            last_slash_index = path.rfind('/')\n",
    "            # Extract the part after the last '/' (excluding '/')\n",
    "            file_name = path[last_slash_index + 1:]\n",
    "            file_name_building = f\"C:/Users/Dila Ozberkman/Desktop/AMS Research/Urban Shade/throwing_shade/data/clean_data/solar/{osmid}/rdy_for_processing/\" + file_name[:-7] + \"building_dsm.tif\"\n",
    "            file_name_trees = f\"C:/Users/Dila Ozberkman/Desktop/AMS Research/Urban Shade/throwing_shade/data/clean_data/solar/{osmid}/rdy_for_processing/\" + file_name[:-7] + \"canopy_dsm.tif\"\n",
    "            # file_name_building = f'../data/clean_data/solar/{osmid}/rdy_for_processing/' + file_name[:-7] + \"building_dsm.tif\"\n",
    "            # file_name_trees = f'../data/clean_data/solar/{osmid}/rdy_for_processing/' + file_name[:-7] + \"canopy_dsm.tif\"\n",
    "\n",
    "            # processing_directory = f'../data/clean_data/solar/{osmid}/rdy_for_processing/'\n",
    "            processing_directory = f\"C:/Users/Dila Ozberkman/Desktop/AMS Research/Urban Shade/throwing_shade/data/clean_data/solar/{osmid}/rdy_for_processing/\"\n",
    "\n",
    "            directory_check(directory=processing_directory, shadow_check=False)\n",
    "\n",
    "\n",
    "            # Replace nan values with 0 for canopy raster:\n",
    "            canopy_dsm = np.nan_to_num(canopy_dsm, nan=0)\n",
    "\n",
    "            n = 50\n",
    "\n",
    "            crop_and_save_raster(canopy_dsm, dsm_transform, dsm_meta, nodata_value, n,file_name_trees)\n",
    "            crop_and_save_raster(dsm_buildings, dsm_transform, dsm_meta, nodata_value, n,file_name_building)\n",
    "\n",
    "            print(f\"Original transform: {dsm_transform}\")\n",
    "            print(f\"New transform: {transform}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path} with OSMID: {osmid}: {e}\")\n",
    "\n",
    "# Function to apply a median filter to a raster dataset\n",
    "def apply_median_filter(data, nodata_value, size=3, nodata=True):\n",
    "\n",
    "    if nodata:\n",
    "        # Create a mask for nodata values\n",
    "        mask = (data == nodata_value)\n",
    "\n",
    "        # Apply the median filter only to valid data\n",
    "        filtered_data = data.copy()\n",
    "        filtered_data[~mask] = median_filter(data[~mask], size=size)\n",
    "        print('Filtering: Ignoring Nodata')\n",
    "    else:\n",
    "        filtered_data = data.copy()\n",
    "        filtered_data = median_filter(data, size=size)\n",
    "        print('Filtering; Handling nodata')\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "# Function to apply a mean filter to a raster dataset\n",
    "def apply_mean_filter(data, nodata_value, size=3):\n",
    "    # Create a mask for nodata values\n",
    "    mask = (data == nodata_value)\n",
    "\n",
    "    # Apply the mean filter only to valid data\n",
    "    filtered_data = data.copy()\n",
    "    filtered_data[~mask] = uniform_filter(data[~mask], size=size)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "# Function to apply a Gaussian filter to a raster dataset\n",
    "def apply_gaussian_filter(data, nodata_value, sigma=1):\n",
    "    # Create a mask for nodata values\n",
    "    mask = (data == nodata_value)\n",
    "\n",
    "    # Apply the Gaussian filter only to valid data\n",
    "    filtered_data = data.copy()\n",
    "    filtered_data[~mask] = gaussian_filter(data[~mask], sigma=sigma)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "# Function to apply a minimum filter to a raster dataset\n",
    "def apply_minimum_filter(data, nodata_value, size=3, nodata=True):\n",
    "\n",
    "    if nodata:\n",
    "        # Create a mask for nodata values\n",
    "        mask = (data == nodata_value)\n",
    "\n",
    "        # Apply the Gaussian filter only to valid data\n",
    "        filtered_data = data.copy()\n",
    "        filtered_data[~mask] = minimum_filter(data[~mask], size=size)\n",
    "        print('Filtering: Ignoring Nodata')\n",
    "    else:\n",
    "        filtered_data = data.copy()\n",
    "        filtered_data = minimum_filter(data, size=size)\n",
    "        print('Filtering; Handling nodata')\n",
    "\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "def crop_and_save_raster(raster, transform, meta, nodata, n, out_path):\n",
    "    # TODO: MAYBE JUST REPLACE THE NAN WITH MIN INSTEAD OF CROPPING?\n",
    "    print(f\"Before cropping: {raster.shape}\")\n",
    "    # Calculate new top-left corner coordinates\n",
    "    new_x = transform.c + n * transform.a\n",
    "    new_y = transform.f + n * transform.e\n",
    "\n",
    "    # Calculate new transformation matrix\n",
    "    new_transform = Affine(transform.a, transform.b, new_x,\n",
    "                        transform.d, transform.e, new_y)\n",
    "\n",
    "    # Crop the data by removing n pixels from each edge\n",
    "    cropped_data = raster[n:-n, n:-n]\n",
    "\n",
    "    # Find the minimum value of the non-NaN elements\n",
    "    min_value = np.nanmin(cropped_data)\n",
    "\n",
    "    # Fill NaN values with the minimum value\n",
    "    cropped_data = np.where(np.isnan(cropped_data), min_value, cropped_data)\n",
    "\n",
    "    print(f\"After cropping: {cropped_data.shape}\")\n",
    "\n",
    "    # Update the metadata\n",
    "    meta.update({\n",
    "        'height': cropped_data.shape[0],\n",
    "        'width': cropped_data.shape[1],\n",
    "        'transform': new_transform\n",
    "    })\n",
    "\n",
    "    # Save the cropped raster data\n",
    "    with rasterio.open(out_path, 'w', **meta) as dst:\n",
    "        dst.write(cropped_data, 1)\n",
    "        if nodata is not None:\n",
    "            dst.nodata = nodata\n",
    "\n",
    "    print(f\"Cropped raster saved to {out_path}\")\n",
    "\n",
    "def check_files_exist(file_paths):\n",
    "    \"\"\"\n",
    "    Check if all files in the list exist.\n",
    "\n",
    "    Parameters:\n",
    "    file_paths (list of str): List of file paths to check.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if all files exist, False otherwise.\n",
    "    \"\"\"\n",
    "    return all(os.path.exists(file_path) for file_path in file_paths)\n",
    "\n",
    "### The\n",
    "# Function to extract the identifier from the file path\n",
    "def extract_identifier(path):\n",
    "    # Extract the last segment of the path\n",
    "    last_segment = path.split('/')[-1]\n",
    "\n",
    "    # Use regular expression to match the pattern before _20xx_\n",
    "    match = re.match(r'(.*)_20\\d{2}_', last_segment)\n",
    "\n",
    "    if match:\n",
    "        identifier = match.group(1)\n",
    "    else:\n",
    "        identifier = last_segment.split('_20')[0]\n",
    "\n",
    "    return identifier\n",
    "\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "def directory_check(directory, shadow_check=True, shade_intervals=False, date=dt.datetime.now()):\n",
    "    \"\"\"\n",
    "    Checks if a directory exists and optionally verifies the presence of shadow fraction files.\n",
    "\n",
    "    If the directory does not exist, it is created. If `shadow_check` is enabled, the function\n",
    "    searches for files containing 'shadow_fraction_on_' followed by the given date. If\n",
    "    `shade_intervals` is provided as a list of datetime objects, it returns a list of booleans\n",
    "    indicating whether a file exists for each interval.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    directory : str\n",
    "        The path to the directory to check or create.\n",
    "    shadow_check : bool, optional\n",
    "        Whether to check for shadow fraction files (default is True).\n",
    "    shade_intervals : list of datetime, optional\n",
    "        A list of datetime objects representing specific intervals to check for shadow fraction files.\n",
    "    date : datetime, optional\n",
    "        The reference date for file checking (default is the current date).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    bool or list of bool\n",
    "        - If `shade_intervals` is not provided, returns True if at least one shadow fraction file is found,\n",
    "          otherwise returns False.\n",
    "        - If `shade_intervals` is provided, returns a list of booleans where each element corresponds to whether\n",
    "          a shadow fraction file exists for a specific interval.\n",
    "\n",
    "    Appends False if it doesn't exist, True if it exists\n",
    "    returns list if shade_intervals exist, True or False if it doesn't\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory {directory} created.\")\n",
    "    else:\n",
    "        print(f\"Directory {directory} already exists.\")\n",
    "\n",
    "    # Convert date to string format\n",
    "    timestr = date.strftime(\"%Y%m%d\")\n",
    "    print(\"Timestr used for date: \", timestr)\n",
    "\n",
    "    if shadow_check:\n",
    "        # Check for files containing 'shadow_fraction_on_' with the given date\n",
    "        shadow_files = [f for f in os.listdir(directory) if f'shadow_fraction_on_{timestr}' in f]\n",
    "\n",
    "        print(\"these are the shadow files: \", shadow_files)\n",
    "\n",
    "        if shadow_files:\n",
    "            if shade_intervals:\n",
    "                # Ensure shade_intervals is a list of datetime objects\n",
    "                if not isinstance(shade_intervals, list) or not all(isinstance(ts, dt.datetime) for ts in shade_intervals):\n",
    "                    raise ValueError(\"shade_intervals must be a list of datetime objects.\")\n",
    "\n",
    "                shade_int_check = []\n",
    "                for interval in shade_intervals:\n",
    "                    int_time = interval.strftime(\"%Y%m%d_%H%M\")\n",
    "                    shadow_files_interval = [f for f in shadow_files if f'shadow_fraction_on_{int_time}' in f]\n",
    "                    if shadow_files_interval:\n",
    "                        print(f\"File containing 'shadow_fraction_on_{int_time}' found: {shadow_files_interval}\")\n",
    "                        shade_int_check.append(True)\n",
    "                    else:\n",
    "                        print(f\"No files containing 'shadow_fraction_on_{int_time}' found.\")\n",
    "                        shade_int_check.append(False)\n",
    "                return shade_int_check\n",
    "            else:\n",
    "                print(f\"Files containing 'shadow_fraction_on_{timestr}' found: {shadow_files}\")\n",
    "                return True  # Required files found\n",
    "        else:\n",
    "            print(f\"No files containing 'shadow_fraction_on_{timestr}' found.\")\n",
    "            return False  # Required files not found\n",
    "\n",
    "def filter_intervals(intervals, building_shadow_files_exist, tree_shadow_files_exist):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    # filter to only calculate intervals that don't have a file\n",
    "    if isinstance(building_shadow_files_exist, list):\n",
    "        assert len(intervals) == len(building_shadow_files_exist), \"Directory check for the intervals is broken\"\n",
    "        building_intervals_needed = [intervals[i] for i, check in enumerate(building_shadow_files_exist) if not check]\n",
    "        if len(building_intervals_needed) < 1:\n",
    "            building_intervals_needed = False\n",
    "    elif not building_shadow_files_exist:\n",
    "        building_intervals_needed = intervals\n",
    "    else:\n",
    "        building_intervals_needed = False\n",
    "\n",
    "    if isinstance(tree_shadow_files_exist, list):\n",
    "        assert len(intervals) == len(tree_shadow_files_exist), \"Directory check for the intervals is broken\"\n",
    "        tree_intervals_needed = [intervals[i] for i, check in enumerate(tree_shadow_files_exist) if not check]\n",
    "        if len(tree_intervals_needed) < 1:\n",
    "            tree_intervals_needed = False\n",
    "    elif not tree_shadow_files_exist:\n",
    "        tree_intervals_needed = intervals\n",
    "    else:\n",
    "        tree_intervals_needed = False\n",
    "\n",
    "    return building_intervals_needed, tree_intervals_needed\n",
    "\n",
    "def shade_processing(bldg_path, matched_chm_path, osmid, date, shade_interval, timestamps, start_time, inputs, combined, building):\n",
    "    \"\"\"\n",
    "    Process shade simulation for a given pair of building and canopy DSM files.\n",
    "\n",
    "    Parameters:\n",
    "        bldg_path (str): File path to the building DSM raster file.\n",
    "        matched_chm_path (str): File path to the matched canopy DSM raster file.\n",
    "        osmid (str): The OpenStreetMap identifier used to locate the dataset.\n",
    "        date (datetime.date): The date for which the shade simulation is executed.\n",
    "        shade_interval (int): The time interval (in minutes) used in the shade simulation.\n",
    "        timestamps (list or tuple): A two-element structure where:\n",
    "                                    - Element 0 is the final timestamp.\n",
    "                                    - Element 1 is a list of intermediate timestamps.\n",
    "                                    If the list of intermediate timestamps is empty, it is treated as False.\n",
    "        inputs (dict): A dictionary of additional simulation parameters, typically including:\n",
    "                       - 'utc': UTC offset.\n",
    "                       - 'dst': Daylight saving time offset.\n",
    "                       - 'trs': Transmissivity value.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    def run_building_shade(inputs):\n",
    "        shade_bldg = shade.shadecalculation_setup(\n",
    "                    filepath_dsm=bldg_path,\n",
    "                    filepath_veg=matched_chm_path,\n",
    "                    tile_no=tile_no,\n",
    "                    date=date,\n",
    "                    intervalTime=shade_interval,\n",
    "                    final_stamp=final_stamp,\n",
    "                    start_time=start_time,\n",
    "                    shade_fractions=building_intervals_needed,\n",
    "                    onetime=0,\n",
    "                    filepath_save=building_directory,\n",
    "                    UTC=inputs['utc'],\n",
    "                    dst=inputs['dst'],\n",
    "                    useveg=0,\n",
    "                    trunkheight=25,\n",
    "                    # CHANGED TRANSMISSIVITY from 15 to 10 percent\n",
    "                    transmissivity=inputs['trs']\n",
    "                )\n",
    "\n",
    "    def run_tree_shade(inputs):\n",
    "        shade_veg = shade.shadecalculation_setup(\n",
    "            filepath_dsm=bldg_path,\n",
    "            filepath_veg=matched_chm_path,\n",
    "            tile_no=tile_no,\n",
    "            date=date,\n",
    "            intervalTime=shade_interval,\n",
    "            final_stamp=final_stamp,\n",
    "            start_time=start_time,\n",
    "            shade_fractions=tree_intervals_needed,\n",
    "            onetime=0,\n",
    "            filepath_save=tree_directory,\n",
    "            UTC=inputs['utc'],\n",
    "            dst=inputs['dst'],\n",
    "            useveg=1,\n",
    "            trunkheight=25,\n",
    "            transmissivity=inputs['trs']\n",
    "        )\n",
    "\n",
    "\n",
    "    final_stamp, intervals = timestamps[0], timestamps[1]\n",
    "\n",
    "    if final_stamp is not None:\n",
    "        date = final_stamp\n",
    "    else:\n",
    "        date = datetime.combine(date, datetime.min.time()).replace(hour=23, minute=59, second=59)\n",
    "\n",
    "    if not intervals:\n",
    "        intervals = False\n",
    "\n",
    "    bldg_path = bldg_path.replace(\"\\\\\", \"/\")\n",
    "    matched_chm_path = matched_chm_path.replace(\"\\\\\", \"/\")\n",
    "    identifier = extract_identifier(bldg_path)\n",
    "\n",
    "    print(f\"This is the building path I am looking at: {bldg_path}, This is the matched canopy path I am looking at: {matched_chm_path}\")\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(matched_chm_path):\n",
    "        print(f\"The file {matched_chm_path} exists.\")\n",
    "    else:\n",
    "        print(f\"The file {matched_chm_path} does not exist.\")\n",
    "\n",
    "    # Create directories\n",
    "    folder_no = identifier.split('_')[-1]\n",
    "    folder_no = '/' + folder_no\n",
    "    tile_no = identifier\n",
    "    # tile_no = '/' + identifier\n",
    "\n",
    "    print(\"Tile no or identifier:\", tile_no)\n",
    "\n",
    "    building_directory = f\"C:/Users/Dila Ozberkman/Desktop/AMS Research/Urban Shade/throwing_shade/code/results/output/{osmid}/building_shade/{folder_no}/\"\n",
    "    tree_directory = f\"C:/Users/Dila Ozberkman/Desktop/AMS Research/Urban Shade/throwing_shade/code/results/output/{osmid}/tree_shade/{folder_no}/\"\n",
    "    # building_directory = f'../results/output/{osmid}/building_shade{folder_no}/'\n",
    "    # tree_directory = f'../results/output/{osmid}/tree_shade{folder_no}/'\n",
    "\n",
    "    # if shade_intervals is not empty, the return here is a list of booleans whether the file exists for each interval\n",
    "    # if it is empty it is a single boolean for the final timestamp basically\n",
    "    # TODO: Maybe should I add the final_stamp to the interval check somehow\n",
    "    building_shadow_files_exist = directory_check(building_directory, shadow_check=True, shade_intervals=intervals, date=date)\n",
    "    tree_shadow_files_exist = directory_check(tree_directory, shadow_check=True, shade_intervals=intervals, date=date)\n",
    "\n",
    "    print(\"These are the intervals I need to calculate for: \", intervals)\n",
    "    print(\"These are the building shade directory check for these intervals: \", building_shadow_files_exist)\n",
    "\n",
    "    if intervals:\n",
    "        # filter to only calculate intervals that don't have a file\n",
    "        building_intervals_needed, tree_intervals_needed = filter_intervals(intervals, building_shadow_files_exist, tree_shadow_files_exist)\n",
    "        print(tree_intervals_needed)\n",
    "    else:\n",
    "        building_intervals_needed = False\n",
    "        tree_intervals_needed = False\n",
    "\n",
    "    if building:\n",
    "        print(\"Processing building shade...\")\n",
    "        if not building_shadow_files_exist:\n",
    "            run_building_shade(inputs)\n",
    "        elif isinstance(building_shadow_files_exist, list):\n",
    "            if not all(building_shadow_files_exist):\n",
    "                run_building_shade(inputs)\n",
    "\n",
    "    if combined:\n",
    "        print(\"Processing tree shade...\")\n",
    "        if not tree_shadow_files_exist:\n",
    "            run_tree_shade(inputs)\n",
    "        elif isinstance(tree_shadow_files_exist, list):\n",
    "            if not all(tree_shadow_files_exist):\n",
    "                run_tree_shade(inputs)\n",
    "\n",
    "def reproject_raster_to_dsm(src_raster, src_transform, src_crs, dst_crs, dst_transform, dst_shape):\n",
    "    \"\"\"\n",
    "    Reproject a raster to the DSM's CRS.\n",
    "\n",
    "    The CHM might be in a different CRS to the DSM.\n",
    "\n",
    "    This function reprojects the CHM to the DSM crs.\n",
    "    \"\"\"\n",
    "    dst_raster = np.empty(dst_shape, dtype=src_raster.dtype)\n",
    "    reproject(\n",
    "        source=src_raster,\n",
    "        destination=dst_raster,\n",
    "        src_transform=src_transform,\n",
    "        src_crs=src_crs,\n",
    "        dst_transform=dst_transform,\n",
    "        dst_crs=dst_crs,\n",
    "        resampling=Resampling.nearest\n",
    "    )\n",
    "    return dst_raster\n",
    "\n",
    "def update_mask_within_extent(raster_path, combined_mask, dsm_bounds, dsm_transform, dsm_crs, dsm_shape):\n",
    "    \"\"\"\n",
    "    This function reflects the fact that each DSM tile only overlaps the CHM partially, or overlaps multiple CHM tiles.\n",
    "\n",
    "    We already have a list of CHM tiles that overlap.\n",
    "\n",
    "    In this function we open the tile(s), reproject, and compute the window in which the CHM overlaps with the DSM\n",
    "\n",
    "    Then we use the data in the window to create a raster mask for the original DSM\"\"\"\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Reproject the entire additional raster to the DSM's CRS\n",
    "        src_data = src.read(1)\n",
    "        src_transform = src.transform\n",
    "        src_crs = src.crs\n",
    "\n",
    "        reprojected_data = reproject_raster_to_dsm(\n",
    "            src_data, src_transform, src_crs, dsm_crs, dsm_transform, dsm_shape\n",
    "        )\n",
    "\n",
    "        # Compute the window of the reprojected raster that overlaps with the DSM extent\n",
    "        window = from_bounds(\n",
    "            left=dsm_bounds.left, bottom=dsm_bounds.bottom,\n",
    "            right=dsm_bounds.right, top=dsm_bounds.top,\n",
    "            transform=dsm_transform\n",
    "        )\n",
    "\n",
    "        # Convert window indices to integers\n",
    "        row_off = int(window.row_off)\n",
    "        col_off = int(window.col_off)\n",
    "        height = int(window.height)\n",
    "        width = int(window.width)\n",
    "\n",
    "        # Extract the overlapping window from the reprojected raster\n",
    "        window_data = reprojected_data[\n",
    "            row_off:row_off + height,\n",
    "            col_off:col_off + width\n",
    "        ]\n",
    "\n",
    "        # Create a mask from the reprojected raster within the window\n",
    "        additional_mask = window_data > 0  # Example condition to create a mask from the additional raster\n",
    "\n",
    "        # Update the combined mask using the window's index\n",
    "        row_start, col_start = row_off, col_off\n",
    "        row_end, col_end = row_start + additional_mask.shape[0], col_start + additional_mask.shape[1]\n",
    "\n",
    "        # Ensure the indices are within the bounds of the combined mask\n",
    "        if row_start < 0 or col_start < 0 or row_end > combined_mask.shape[0] or col_end > combined_mask.shape[1]:\n",
    "            print(f\"Skipping {raster_path} due to out of bounds indices\")\n",
    "            return\n",
    "\n",
    "        combined_mask[row_start:row_end, col_start:col_end] |= additional_mask\n",
    "\n",
    "def get_earliest_timestamp(directory, date_obj):\n",
    "    \"\"\"\n",
    "    Finds the earliest timestamp from raster filenames in a directory\n",
    "    that match the given date.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): Path to the directory containing the raster files.\n",
    "    - date_obj (datetime): The reference date.\n",
    "\n",
    "    Returns:\n",
    "    - datetime: The earliest timestamp for the given date, or None if no match is found.\n",
    "    \"\"\"\n",
    "    # print(f\"Trying to find first shade in this directory {directory}\")\n",
    "\n",
    "    date_str = date_obj.strftime(\"%Y%m%d\")  # Convert date to string format YYYYMMDD\n",
    "    pattern = re.compile(r\".*_(\\d{8})_(\\d{4})_LST\\.tif\")  # Regex to match date & time in filename\n",
    "\n",
    "    timestamps = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".tif\") and not filename.endswith(\".tif.ovr\"):  # Ensure only `.tif` files, exclude `.tif.ovr`\n",
    "            match = pattern.match(filename)\n",
    "            if match:\n",
    "                file_date, file_time = match.groups()\n",
    "                if file_date == date_str:  # Check if the date matches\n",
    "                    timestamp = datetime.strptime(f\"{file_date} {file_time}\", \"%Y%m%d %H%M\")\n",
    "                    timestamps.append(timestamp)\n",
    "\n",
    "    return min(timestamps) if timestamps else None\n",
    "\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def get_shade_files_in_range(base_path, shade_type, tile_number, osmid, start_hour, rounded_timestamp):\n",
    "    \"\"\"\n",
    "    Get all tree shade files in a directory within the range of start_hour and rounded_timestamp (inclusive).\n",
    "\n",
    "    Parameters:\n",
    "        base_path (str): The base directory where tree shade files are stored.\n",
    "        tile_number (str): The tile number for shade calculations.\n",
    "        osmid (str): The unique ID for the dataset.\n",
    "        start_hour (datetime): The lower bound timestamp (inclusive).\n",
    "        rounded_timestamp (datetime): The upper bound timestamp (inclusive).\n",
    "\n",
    "    Returns:\n",
    "        list: List of full file paths that fall within the specified time range.\n",
    "    \"\"\"\n",
    "    # Directory containing the tree shade files\n",
    "    directory = f\"{base_path}/{shade_type}/{tile_number}/\"\n",
    "\n",
    "    # Ensure directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "        return []\n",
    "\n",
    "    # Regex pattern to extract timestamp from filenames\n",
    "    pattern = re.compile(rf\"{osmid}_p_{tile_number}_Shadow_(\\d{{8}}_\\d{{4}})_LST\\.tif\")\n",
    "\n",
    "    # List all files in directory\n",
    "    all_files = os.listdir(directory)\n",
    "\n",
    "    # Filter and extract timestamps\n",
    "    valid_files = []\n",
    "    for filename in all_files:\n",
    "        if filename.endswith(\".tif\") and not filename.endswith(\".tif.ovr\"):  # Ensure only `.tif` files, exclude `.tif.ovr`\n",
    "            match = pattern.search(filename)\n",
    "            if match:\n",
    "                file_timestamp_str = match.group(1)  # Extract timestamp string\n",
    "                file_timestamp = datetime.strptime(file_timestamp_str, \"%Y%m%d_%H%M\")  # Convert to datetime\n",
    "\n",
    "                # Check if the timestamp is within the range (inclusive)\n",
    "                if start_hour <= file_timestamp <= rounded_timestamp:\n",
    "                    # print(f\"Start hour: {start_hour}, file timstamp: {file_timestamp}, rounded timestamp: {rounded_timestamp}\")\n",
    "                    valid_files.append(os.path.join(directory, filename))\n",
    "\n",
    "    return sorted(valid_files)  # Return sorted list of file paths\n",
    "\n",
    "\n",
    "def get_closest_shade_file(base_path, shade_type, tile_number, osmid, start_hour):\n",
    "    \"\"\"\n",
    "    Get the closest existing shade file to `start_hour`.\n",
    "    If two timestamps are equidistant, choose the later one.\n",
    "\n",
    "    Parameters:\n",
    "        base_path (str): The base directory where tree shade files are stored.\n",
    "        tile_number (str): The tile number for shade calculations.\n",
    "        osmid (str): The unique ID for the dataset.\n",
    "        start_hour (datetime): The target timestamp.\n",
    "\n",
    "    Returns:\n",
    "        str: Full file path of the closest shade file, or None if no files exist.\n",
    "    \"\"\"\n",
    "    directory = f\"{base_path}/{shade_type}/{tile_number}/\"\n",
    "\n",
    "    # Ensure directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "        return None\n",
    "\n",
    "    # Regex pattern to extract timestamp from filenames\n",
    "    pattern = re.compile(rf\"{osmid}_p_{tile_number}_Shadow_(\\d{{8}}_\\d{{4}})_LST\\.tif\")\n",
    "\n",
    "    # List all files in directory\n",
    "    all_files = os.listdir(directory)\n",
    "\n",
    "    # Extract timestamps from filenames\n",
    "    timestamps = []\n",
    "    file_map = {}  # Dictionary to map timestamps to filenames\n",
    "    for filename in all_files:\n",
    "        if filename.endswith(\".tif\") and not filename.endswith(\".tif.ovr\"):\n",
    "            match = pattern.search(filename)\n",
    "            if match:\n",
    "                file_timestamp_str = match.group(1)  # Extract timestamp string\n",
    "                file_timestamp = datetime.strptime(file_timestamp_str, \"%Y%m%d_%H%M\")  # Convert to datetime\n",
    "                timestamps.append(file_timestamp)\n",
    "                file_map[file_timestamp] = os.path.join(directory, filename)\n",
    "\n",
    "    # If no valid timestamps were found\n",
    "    if not timestamps:\n",
    "        print(f\"No valid shade files found in {directory}\")\n",
    "        return None\n",
    "\n",
    "    # Sort timestamps\n",
    "    timestamps.sort()\n",
    "\n",
    "    # Find the closest timestamp\n",
    "    closest_timestamp = min(\n",
    "        timestamps,\n",
    "        key=lambda t: (abs((t - start_hour).total_seconds()), -t.timestamp())  # Prioritize later timestamps\n",
    "    )\n",
    "\n",
    "    return file_map[closest_timestamp]\n",
    "\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "def extract_datetime_from_path(file_path):\n",
    "    \"\"\"\n",
    "    Extracts the datetime object from the given file path.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The full file path of the raster.\n",
    "\n",
    "    Returns:\n",
    "    - datetime: Extracted datetime object.\n",
    "    \"\"\"\n",
    "    # Extract filename\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    # Regex pattern to find the date and time in the filename\n",
    "    match = re.search(r\"_Shadow_(\\d{8})_(\\d{4})_LST\\.tif\", filename)\n",
    "\n",
    "    if match:\n",
    "        date_part = match.group(1)  # '20230823'\n",
    "        time_part = match.group(2)  # '1200'\n",
    "\n",
    "        # Convert to datetime object\n",
    "        return datetime.strptime(date_part + time_part, \"%Y%m%d%H%M\")\n",
    "\n",
    "    # Return None if no match is found\n",
    "    return None\n",
    "\n",
    "\n",
    "def hours_before_shadow_fr(dataset, base_path, building_mask_path, shade_type, rounded_timestamp, tile_number, osmid, hours_before, buffer):\n",
    "    \"\"\"\n",
    "    Computes the average shadow fraction for all points in the dataset over a specified time range before the rounded timestamp.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (GeoDataFrame): The dataset containing point geometries.\n",
    "    - base_path (str): The root directory where the shadow files are stored.\n",
    "    - shade_type (str): Type of shade to consider (e.g., \"tree_shade\" or \"building_shade\").\n",
    "    - rounded_timestamp (datetime.datetime): The timestamp for which the shadow fraction is being computed.\n",
    "    - osmid (str): The unique identifier for the dataset.\n",
    "    - hours_before (int or float): The number of hours before `rounded_timestamp` to consider for shadow fraction calculation.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: An array of computed shadow fractions for all points in the dataset.\n",
    "    \"\"\"\n",
    "    # Compute the starting timestamp based on hours_before\n",
    "    start_hour = rounded_timestamp - timedelta(hours=hours_before)  # Ensure `hours_before` supports floats\n",
    "\n",
    "    # print(f\"rounded timestamp: {rounded_timestamp}\")\n",
    "\n",
    "    # Get the earliest available shadow file timestamp for the given day\n",
    "    first_shade_time = get_earliest_timestamp(f\"{base_path}/{shade_type}/{tile_number}\", rounded_timestamp)\n",
    "\n",
    "    # print(f\"first shade timestamp: {first_shade_time}\")\n",
    "\n",
    "    if first_shade_time is None:\n",
    "        raise Exception(\"There are no shade files in the directory for this date\")\n",
    "\n",
    "    # Handle case where start_hour is before the first available shadow timestamp\n",
    "    if start_hour <= first_shade_time:\n",
    "        print(\"Start_hour is earlier or the same as first_shade_time, adjusting to first available time.\")\n",
    "        start_hour = first_shade_time\n",
    "\n",
    "        # Construct the path to the shadow fraction raster file for `rounded_timestamp`\n",
    "        timestamp_shadow_fraction_raster = f\"{base_path}/{shade_type}/{tile_number}/{osmid}_p_{tile_number}_shadow_fraction_on_{rounded_timestamp.strftime('%Y%m%d_%H%M')}.tif\"\n",
    "\n",
    "        # If the shadow fraction raster file exists, extract values for all points\n",
    "        return extract_values_from_raster(timestamp_shadow_fraction_raster, building_mask_path, dataset, buffer)  # Direct return if available\n",
    "\n",
    "    # If the exact `start_hour` shadow file doesn't exist, find the closest valid one\n",
    "    shadow_file_path = f\"{base_path}/{shade_type}/{tile_number}/{osmid}_p_{tile_number}_Shadow_{start_hour.strftime('%Y%m%d_%H%M')}_LST.tif\"\n",
    "\n",
    "    if not os.path.exists(shadow_file_path):\n",
    "        print(f\"This shade file for start hour doesn't exist: {shadow_file_path}\")\n",
    "        start_hour_file = get_closest_shade_file(base_path, shade_type, tile_number, osmid, start_hour)\n",
    "        start_hour = extract_datetime_from_path(start_hour_file)\n",
    "        print(f\"This is the new start hour: {start_hour}\")\n",
    "        if start_hour >= rounded_timestamp:\n",
    "            # there are no shade files available\n",
    "            return np.full(len(dataset), np.nan)\n",
    "\n",
    "    # Retrieve all shadow files within the time range [start_hour, rounded_timestamp]\n",
    "    shade_files_for_shadow_frac = get_shade_files_in_range(base_path, shade_type, tile_number, osmid, start_hour, rounded_timestamp)\n",
    "\n",
    "    if not shade_files_for_shadow_frac:\n",
    "        raise Exception(\"Didn't find shade files between start time and timestamp\")\n",
    "\n",
    "    # Compute the shadow fraction by averaging the extracted values from all retrieved shade rasters\n",
    "    shadow_values = np.zeros(len(dataset))\n",
    "\n",
    "    for shade_raster in shade_files_for_shadow_frac:\n",
    "        raster_values = extract_values_from_raster(shade_raster, building_mask_path, dataset, buffer)\n",
    "        shadow_values += np.nan_to_num(raster_values)  # Ensure NaN values don't affect summation\n",
    "\n",
    "    # Compute the final shadow fraction (average)\n",
    "    shadow_fractions = shadow_values / len(shade_files_for_shadow_frac)\n",
    "\n",
    "    return shadow_fractions\n",
    "\n",
    "def extract_values_from_raster(raster_path, building_mask_path, dataset, buffer=0):\n",
    "    \"\"\"\n",
    "    Extracts shade (or similar raster) values at each point location in a dataset, optionally\n",
    "    averaging over a surrounding buffer, and excluding areas covered by buildings.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    raster_path : str\n",
    "        Path to the main raster file containing shade (or other) values.\n",
    "\n",
    "    building_mask_path : str\n",
    "        Path to a building mask raster file. Building pixels are assumed to have value 1.\n",
    "\n",
    "    dataset : GeoDataFrame\n",
    "        GeoDataFrame containing point geometries at which to extract values.\n",
    "\n",
    "    buffer : float, optional (default = 0)\n",
    "        Buffer radius in meters. If greater than 0, the average raster value\n",
    "        is computed over a square window of surrounding pixels.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array of flipped raster values (`1 - value`) for each point.\n",
    "        Returns NaN for:\n",
    "        - Invalid raster coordinates,\n",
    "        - Nodata raster values,\n",
    "        - Points on buildings (for `buffer=0`), or\n",
    "        - Buffers fully covered by buildings/nodata.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(raster_path):\n",
    "        print(f\"Warning: Raster file {raster_path} not found.\")\n",
    "        return np.full(len(dataset), np.nan)\n",
    "\n",
    "    with rasterio.open(raster_path) as src, rasterio.open(building_mask_path) as bsrc:\n",
    "        raster_data = src.read(1, masked=False)\n",
    "        building_mask = bsrc.read(1, masked=False)\n",
    "\n",
    "        raster_nodata = src.nodata if src.nodata is not None else np.nan\n",
    "        building_nodata = bsrc.nodata if bsrc.nodata is not None else np.nan\n",
    "\n",
    "        raster_transform = src.transform\n",
    "        building_transform = bsrc.transform\n",
    "\n",
    "        # Reproject points to match raster CRS\n",
    "        dataset = dataset.to_crs(src.crs)\n",
    "        dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "        values = np.full(len(dataset), np.nan)\n",
    "\n",
    "        res_x, _ = src.res\n",
    "        buffer_pixels = int(buffer / res_x) if buffer > 0 else 0\n",
    "\n",
    "        for idx, row in dataset.iterrows():\n",
    "            x, y = row.geometry.x, row.geometry.y\n",
    "            record_id = row['RECORD']\n",
    "\n",
    "            try:\n",
    "                raster_row, raster_col = rowcol(raster_transform, x, y)\n",
    "                building_row, building_col = rowcol(building_transform, x, y)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error converting coordinates for RECORD {record_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Check bounds before accessing raster/building arrays\n",
    "            if not (0 <= raster_row < raster_data.shape[0] and 0 <= raster_col < raster_data.shape[1]):\n",
    "                print(f\"âŒ Raster index out of bounds for RECORD {record_id}\")\n",
    "                continue\n",
    "            if not (0 <= building_row < building_mask.shape[0] and 0 <= building_col < building_mask.shape[1]):\n",
    "                print(f\"âŒ Building mask index out of bounds for RECORD {record_id}\")\n",
    "                continue\n",
    "\n",
    "            if buffer == 0:\n",
    "                bm_value = building_mask[building_row, building_col]\n",
    "                if bm_value == 1:\n",
    "                    print(f\"ðŸš« Point on building â€” RECORD: {record_id}\")\n",
    "                    values[idx] = np.nan\n",
    "                else:\n",
    "                    val = raster_data[raster_row, raster_col]\n",
    "                    values[idx] = np.nan if val == raster_nodata else val\n",
    "            else:\n",
    "                # Raster window bounds\n",
    "                row_start = max(raster_row - buffer_pixels, 0)\n",
    "                row_end = min(raster_row + buffer_pixels + 1, raster_data.shape[0])\n",
    "                col_start = max(raster_col - buffer_pixels, 0)\n",
    "                col_end = min(raster_col + buffer_pixels + 1, raster_data.shape[1])\n",
    "                raster_window = raster_data[row_start:row_end, col_start:col_end]\n",
    "\n",
    "                # Building mask window bounds\n",
    "                row_start_b = max(building_row - buffer_pixels, 0)\n",
    "                row_end_b = min(building_row + buffer_pixels + 1, building_mask.shape[0])\n",
    "                col_start_b = max(building_col - buffer_pixels, 0)\n",
    "                col_end_b = min(building_col + buffer_pixels + 1, building_mask.shape[1])\n",
    "                building_window = building_mask[row_start_b:row_end_b, col_start_b:col_end_b]\n",
    "\n",
    "                # Match shapes by trimming to smallest size\n",
    "                min_rows = min(raster_window.shape[0], building_window.shape[0])\n",
    "                min_cols = min(raster_window.shape[1], building_window.shape[1])\n",
    "                raster_window = raster_window[:min_rows, :min_cols]\n",
    "                building_window = building_window[:min_rows, :min_cols]\n",
    "\n",
    "                filtered = np.where(\n",
    "                    (building_window == 1) | (raster_window == raster_nodata),\n",
    "                    np.nan,\n",
    "                    raster_window\n",
    "                )\n",
    "                valid_vals = filtered[~np.isnan(filtered)]\n",
    "                if valid_vals.size == 0:\n",
    "                    print(f\"âš ï¸ Empty window after masking â€” RECORD: {record_id}\")\n",
    "                    values[idx] = np.nan\n",
    "                else:\n",
    "                    values[idx] = np.nanmean(valid_vals)\n",
    "\n",
    "\n",
    "    values_flipped = np.where(np.isnan(values), np.nan, 1 - values)\n",
    "    return values_flipped\n",
    "\n",
    "\n",
    "def get_dataset_shaderesult(dataset, osmid, building_shade_step, tree_shade_step,\n",
    "                          bldg_shadow_fraction, tree_shadow_fraction, hours_before,\n",
    "                          buffer, binned):\n",
    "    \"\"\"\n",
    "    Retrieve shade values for all points in the dataset based on tile ID and rounded timestamp.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (GeoDataFrame): Dataset containing point geometries.\n",
    "    - building_shade_step (bool): Whether to include building shade data.\n",
    "    - tree_shade_step (bool): Whether to include tree shade data.\n",
    "    - bldg_shadow_fraction (bool): Whether to include building shadow fraction data.\n",
    "    - tree_shadow_fraction (bool): Whether to include tree shadow fraction data.\n",
    "    - hours_before (int or float, optional): Number of hours before `rounded_timestamp` to consider for shadow fraction calculation.\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame: Dataset with additional columns for shade values.\n",
    "    \"\"\"\n",
    "\n",
    "    tile_id = dataset[\"tile_number\"].unique()[0]\n",
    "    rounded_timestamp = dataset[\"rounded_timestamp\"].unique()[0]\n",
    "    tile_number = tile_id.split(\"_\")[-1]\n",
    "\n",
    "    if binned:\n",
    "        binned_date = dataset[\"binned_date\"].unique()[0]\n",
    "        if isinstance(binned_date, date) and not isinstance(binned_date, datetime):  # Use 'date' and 'datetime' from datetime module\n",
    "            binned_date = datetime.combine(binned_date, time())  # Convert date-only to full datetime\n",
    "            binned_rounded_ts = binned_date.replace(hour=rounded_timestamp.hour, minute=rounded_timestamp.minute, second=rounded_timestamp.second)\n",
    "        else:\n",
    "            binned_rounded_ts = binned_date.replace(hour=rounded_timestamp.hour, minute=rounded_timestamp.minute, second=rounded_timestamp.second)\n",
    "\n",
    "        rounded_ts = binned_rounded_ts\n",
    "\n",
    "    else:\n",
    "        rounded_ts = rounded_timestamp\n",
    "\n",
    "    # Define paths for shade rasters\n",
    "    base_path = f\"C:/Users/Dila Ozberkman/Desktop/AMS Research/Urban Shade/throwing_shade/code/results/output/{osmid}\"\n",
    "\n",
    "    building_shade_path = f\"{base_path}/building_shade/{tile_number}/{osmid}_{tile_id}_Shadow_{rounded_ts.strftime('%Y%m%d_%H%M')}_LST.tif\"\n",
    "    tree_shade_path = f\"{base_path}/tree_shade/{tile_number}/{osmid}_{tile_id}_Shadow_{rounded_ts.strftime('%Y%m%d_%H%M')}_LST.tif\"\n",
    "    bldg_shadow_fraction_path = f\"{base_path}/building_shade/{tile_number}/{osmid}_{tile_id}_shadow_fraction_on_{rounded_ts.strftime('%Y%m%d_%H%M')}.tif\"\n",
    "    tree_shadow_fraction_path = f\"{base_path}/tree_shade/{tile_number}/{osmid}_{tile_id}_shadow_fraction_on_{rounded_ts.strftime('%Y%m%d_%H%M')}.tif\"\n",
    "\n",
    "    # Initialize empty result DataFrame\n",
    "    result_df = pd.DataFrame(index=dataset.index)\n",
    "\n",
    "    # TODO: get building mask path and submit to extract_values_from_raster\n",
    "    building_mask_file = [\n",
    "        bldg_path for bldg_path in glob.glob(os.path.join(f\"../data/clean_data/solar/{osmid}\", '*mask.tif')) if f\"{tile_id}_\" in bldg_path\n",
    "    ]\n",
    "\n",
    "    if not building_mask_file:\n",
    "        raise Exception(\"Couldn't find building mask file to extract shade values\")\n",
    "    else:\n",
    "        building_mask_path = building_mask_file[0]\n",
    "        print(f\"this is the building mask: {building_mask_path} for tile number {tile_id}\")\n",
    "\n",
    "    # Extract values if the respective shade calculations exist\n",
    "    if building_shade_step:\n",
    "        print(\"extracting building shade values...\")\n",
    "        result_df[\"building_shade\"] = extract_values_from_raster(building_shade_path, building_mask_path, dataset, buffer)\n",
    "\n",
    "    if tree_shade_step:\n",
    "        result_df[\"tree_shade\"] = extract_values_from_raster(tree_shade_path, building_mask_path, dataset, buffer)\n",
    "\n",
    "    if bldg_shadow_fraction:\n",
    "        result_df[\"bldg_shadow_fraction\"] = extract_values_from_raster(bldg_shadow_fraction_path, building_mask_path, dataset, buffer)\n",
    "\n",
    "    if tree_shadow_fraction:\n",
    "        result_df[\"tree_shadow_fraction\"] = extract_values_from_raster(tree_shadow_fraction_path, building_mask_path, dataset, buffer)\n",
    "\n",
    "    if hours_before:\n",
    "        for hr_before in hours_before:\n",
    "            # Ensure `hours_before` is a valid number\n",
    "            assert isinstance(hr_before, (int, float)), \"hours_before must be an int or float\"\n",
    "\n",
    "            if tree_shade_step:\n",
    "                result_df[f\"tree_{hr_before}_before_shadow_fraction\"] = hours_before_shadow_fr(\n",
    "                    dataset, base_path, building_mask_path, \"tree_shade\", rounded_ts, tile_number, osmid, hr_before, buffer\n",
    "                )\n",
    "\n",
    "            if building_shade_step:\n",
    "                result_df[f\"bldg_{hr_before}_before_shadow_fraction\"] = hours_before_shadow_fr(\n",
    "                    dataset, base_path, building_mask_path, \"building_shade\", rounded_ts, tile_number, osmid, hr_before, buffer\n",
    "                )\n",
    "\n",
    "    print(f\"Number of columns in the results_df: {len(result_df.columns)}\")\n",
    "\n",
    "    # Merge results back into dataset\n",
    "    dataset_final = pd.concat([dataset, result_df], axis=1)\n",
    "\n",
    "    print(f\"Number of columns in the dataset_final: {len(dataset_final.columns)}\")\n",
    "\n",
    "    return dataset_final\n",
    "\n",
    "def process_subset(timestamp_data, osmid, building_shade_step=False, tree_shade_step=False,\n",
    "                bldg_shadow_fraction=False, tree_shadow_fraction=False, hours_before=[], buffer=0,\n",
    "                binned=False):\n",
    "    \"\"\"Function to process each subset in parallel\"\"\"\n",
    "    return get_dataset_shaderesult(\n",
    "        timestamp_data, osmid,\n",
    "        building_shade_step=building_shade_step,\n",
    "        tree_shade_step=tree_shade_step,\n",
    "        bldg_shadow_fraction=bldg_shadow_fraction,\n",
    "        tree_shadow_fraction=tree_shadow_fraction,\n",
    "        hours_before=hours_before,\n",
    "        buffer=buffer,\n",
    "        binned=binned\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
